## BankinChallenge (rendu le 25 Janvier 2017)

AnaÃ«l CHARDAN aka Nanou ğŸ¤ª, Ã©lÃ¨ve ingÃ©nieur Ã  l'Ã©cole des Mines de Nantes en alternance chez Akeneo.

### Technologies choisies ğŸ’:
- NodeJS (avec transpileur Typescript), le choix de Typescript s'est fait pour avoir une meilleure gestion des types. En effet, n'ayant jamais fait de NodeJS auparavant, avoir un systÃ¨me de type pour m'aider Ã  commencer m'a Ã©tÃ© trÃ¨s utile.
- Chrome Headless avec puppeteer

### Lancement ğŸš€

```
yarn install //pour installer les dÃ©pendances
yarn tsc //pour transpiler en nodejs
yarn scrap //pour scrapper
```

Les transactions rÃ©cupÃ©rÃ©es sont stockeÃ©s dans un fichier `results/transactions.json`

### Architecture ğŸ¢

L'architecture choisie est celle des Ports/Adapters et plus prÃ©cisÃ©ment l'OnionArchitecture qui
permet de bien dissocier le mÃ©tier, les entrÃ©es sorties, trÃ¨s utilisÃ©es dans le Domain Driven Design.

Dossiers:
* `Application`: c'est notre use case, c'est Ã  dire, rÃ©cupÃ©rer toutes les transactions! Il est abstrait de lÃ  oÃ¹ nous allons chercher ces fameuses transactions. Le point d'entrÃ©e est `TransactionsFetcher.js` qui grÃ¢ce Ã  une queue permet de rÃ©cupÃ©rer des transactions en parallÃ¨le, par batch de 30 pages ici. 
* `Domain`: ce sont nos objets mÃ©tiers!, les Transactions par exemple.
* `Infrasctructure`: C'est nos portes d'entrÃ©es et de sorties, autrement appelÃ©s les adapters.

### Comment Ã§a se passe lÃ  dedans? â‰ï¸

Pour scrapper une page, plusieurs scÃ©narios peuvent arrivÃ©s (#coderAvecLesPieds)

- Une _magnifique_ alerte pop, on clique dessus, et on clique sur reload
- Ensuite deux possibilitÃ©s:
  - Le tableau est chargÃ© dans une iframe
  - Le tableau est chargÃ© dans la page directement (ce qui peut aussi arrivÃ© sans la popup)

Le rÃ©sultat est obtenu en environ 23 secondes sachant que des choix ont Ã©tÃ© fait:
- L'Ã©criture dans un fichier (avec trie) est coÃ»teux ~2secondes
- On ne sait potentiellement pas combien de page il faut scrapper (mÃªme si on pourrais assumer le start=5000) et Ã§a c'est coÃ»teux aussi parce que la queue enfile les processus les uns aprÃ¨s les autres et donc va surement empiler des pages Ã  ne pas scrapper (problÃ¨me de l'asynchrone, mais l'on pourrait trÃ¨s bien imaginer annuler  les tÃ¢ches inutiles quand on sait que la derniÃ¨re page a Ã©tÃ© atteinte) ~8secondes

## Le mot de la fin ğŸ˜

Le challenge Ã©tait cool, Ã§a m'a permis de jouer avec NodeJS (ce que je n'avais jamais eu l'occasion) et jouer avec Puppeeter.

Une question, on pourra avoir votre solution? ğŸ˜

Bisous ğŸ˜™